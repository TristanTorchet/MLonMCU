{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset: AFHQ\n",
    "- 1. Training the model\n",
    "- 2. Quantizing the model to full int8\n",
    "- 3. Evaluating the quantized model on the test set\n",
    "- 4. Exporting an image to a text file\n",
    "\n",
    "Details:\n",
    "- load the TensorFlow dataset (train, validation, test)\n",
    "    - convert the TF dataset into a numpy one\n",
    "    - convert the testing dataset into 2 numpy arrays (images and labels) for the on-mcu evaluation\n",
    "- normalize the datasets from uint8 [0, 255] to float32 [0, 1]\n",
    "- convert the TF training and testing datasets into a numpy one\n",
    "- recover from the numpy training dataset the normalized images as a 4D numpy array for quantization (i.e for the representative dataset)\n",
    "- recover from the numpy testing dataset the normalized images as a 4D numpy array and the labesl into 2 separate numpy arrays for the on-mcu evaluation\n",
    "    - convert the TF datasets into numpy ones\n",
    "- Create, build, compile and train the full precision model on the tf training dataset and validation dataset\n",
    "- Evaluate the fp-model on the tensorflow test set\n",
    "- Quantize the model to full int8 and recover the scaling and zero point parameters that converts float32 to int8\n",
    "- Convert the numpy test images to int8 using the scaling and zero point parameters\n",
    "- Save the int8 test images and labels to .npy format for on-mcu evaluation\n",
    "- Evaluate the quantized model on the int8 test images\n",
    "- Export an int8 train image to a text file for STM32 CubeIDE Build using a data.h file. (USING DECIMAL FORMAT, no need to convert to hex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14630 files belonging to 3 classes.\n",
      "Using 13167 files for training.\n",
      "Found 14630 files belonging to 3 classes.\n",
      "Using 1463 files for validation.\n",
      "Found 1500 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "PATH_TO_IMGS = 'dataset_afhq/'\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f'{PATH_TO_IMGS}train',\n",
    "    seed=123,\n",
    "    image_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f'{PATH_TO_IMGS}train',\n",
    "    seed=123,\n",
    "    image_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    f'{PATH_TO_IMGS}val',\n",
    "    seed=123,\n",
    "    image_size=(64, 64),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images_np.shape = (1500, 64, 64, 3)\n",
      "test_images_np.dtype = float32\n",
      "test_labels_np.shape = (1500,)\n",
      "test_labels_np.dtype = int32\n",
      "\n",
      "train_images_np_scaled.shape = (13167, 64, 64, 3)\n",
      "train_images_np_scaled.dtype = float32\n",
      "np.max(train_images_np_scaled) = 1.0\n",
      "np.min(train_images_np_scaled) = 0.0\n",
      "train_images_np_scaled[0,:,:,0] = \n",
      "[[0.3627451  0.35882354 0.377451   ... 0.39901963 0.3794118  0.3745098 ]\n",
      " [0.39411768 0.3745098  0.38529414 ... 0.12352942 0.48333335 0.37058824]\n",
      " [0.40784317 0.45882356 0.6362746  ... 0.11470589 0.4411765  0.3137255 ]\n",
      " ...\n",
      " [0.1892157  0.3401961  0.33137256 ... 0.37352943 0.34607846 0.26666668]\n",
      " [0.22254904 0.327451   0.3372549  ... 0.28333336 0.29901963 0.3137255 ]\n",
      " [0.20490198 0.227451   0.26372552 ... 0.22450982 0.21274512 0.27450982]]\n",
      "\n",
      "test_images_np_scaled.shape = (1500, 64, 64, 3)\n",
      "test_images_np_scaled.dtype = float32\n",
      "test_labels_np.shape = (1500,)\n",
      "test_labels_np.dtype = int32\n"
     ]
    }
   ],
   "source": [
    "# convert the tensorflow dataset to numpy dataset\n",
    "train_ds_np = tfds.as_numpy(train_ds) # needed for quantization\n",
    "test_ds_np = tfds.as_numpy(test_ds)  # needed for testing the quantized model\n",
    "\n",
    "# FOR THE ON-MCU EVALUATION\n",
    "# convert the images to a unique 4D (num_samples, height, width, channels) numpy array\n",
    "# for testing, we need both the images and the labels\n",
    "test_images_np = np.concatenate([x for x, y in test_ds_np], axis=0)\n",
    "print(f\"test_images_np.shape = {test_images_np.shape}\")\n",
    "print(f\"test_images_np.dtype = {test_images_np.dtype}\")\n",
    "test_labels_np = np.concatenate([y for x, y in test_ds_np], axis=0)\n",
    "print(f'test_labels_np.shape = {test_labels_np.shape}')\n",
    "print(f'test_labels_np.dtype = {test_labels_np.dtype}')\n",
    "\n",
    "# save the test data as numpy arrays using UINT8 FORMAT\n",
    "np.save('dataset_afhq/x_test_afhq.npy', test_images_np.astype(np.uint8))\n",
    "np.save('dataset_afhq/y_test_afhq.npy', test_labels_np.astype(np.uint8))\n",
    "\n",
    "# FOR THE OFF-MCU TRAINING\n",
    "# Normalize the data\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# convert the images to a unique 4D (num_samples, height, width, channels) numpy array\n",
    "train_ds_np_scaled = tfds.as_numpy(train_ds)\n",
    "test_ds_np_scaled = tfds.as_numpy(test_ds)\n",
    "# for training, we only need the images\n",
    "train_images_np_scaled = np.concatenate([x for x, y in train_ds_np_scaled], axis=0)\n",
    "print(f'\\ntrain_images_np_scaled.shape = {train_images_np_scaled.shape}')\n",
    "print(f'train_images_np_scaled.dtype = {train_images_np_scaled.dtype}')\n",
    "print(f'np.max(train_images_np_scaled) = {np.max(train_images_np_scaled)}')\n",
    "print(f'np.min(train_images_np_scaled) = {np.min(train_images_np_scaled)}')\n",
    "print(f'train_images_np_scaled[0,:,:,0] = \\n{train_images_np_scaled[0,:,:,0]}') # image 0 channel 0\n",
    "# for testing, we need both the images and the labels\n",
    "test_images_np_scaled = np.concatenate([x for x, y in test_ds_np_scaled], axis=0)\n",
    "print(f'\\ntest_images_np_scaled.shape = {test_images_np_scaled.shape}')\n",
    "print(f'test_images_np_scaled.dtype = {test_images_np_scaled.dtype}')\n",
    "test_labels_np = np.concatenate([y for x, y in test_ds_np], axis=0)\n",
    "print(f'test_labels_np.shape = {test_labels_np.shape}')\n",
    "print(f'test_labels_np.dtype = {test_labels_np.dtype}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_images_np_scaled_float32.shape = (13167, 64, 64, 3)\n",
      "train_images_np_scaled_float32.dtype = float32\n",
      "np.max(train_images_np_scaled_float32) = 1.0\n",
      "np.min(train_images_np_scaled_float32) = 0.0\n",
      "train_images_np_scaled_float32[0,:,:,0] = \n",
      "[[0.02941177 0.02941177 0.02156863 ... 0.01176471 0.01176471 0.01176471]\n",
      " [0.0372549  0.0372549  0.02745098 ... 0.01176471 0.01176471 0.01176471]\n",
      " [0.03921569 0.03921569 0.03137255 ... 0.01176471 0.01176471 0.01176471]\n",
      " ...\n",
      " [0.4647059  0.49313727 0.5764706  ... 0.35980394 0.26666668 0.23529413]\n",
      " [0.4647059  0.49803925 0.57549024 ... 0.3637255  0.34313726 0.34411767]\n",
      " [0.4382353  0.47254905 0.5382353  ... 0.48529413 0.3892157  0.3784314 ]]\n",
      "\n",
      "test_images_np_scaled_float32.shape = (1500, 64, 64, 3)\n",
      "test_images_np_scaled_float32.dtype = float32\n",
      "test_labels_np.shape = (1500,)\n",
      "test_labels_np.dtype = int32\n"
     ]
    }
   ],
   "source": [
    "# FOR THE OFF-MCU TRAINING\n",
    "# Normalize the data\n",
    "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# convert the images to a unique 4D (num_samples, height, width, channels) numpy array\n",
    "train_ds_np_scaled_float32 = tfds.as_numpy(train_ds)\n",
    "test_ds_np_scaled_float32 = tfds.as_numpy(test_ds)\n",
    "# for training, we only need the images\n",
    "train_images_np_scaled_float32 = np.concatenate([x for x, y in train_ds_np_scaled_float32], axis=0)\n",
    "print(f'\\ntrain_images_np_scaled_float32.shape = {train_images_np_scaled_float32.shape}')\n",
    "print(f'train_images_np_scaled_float32.dtype = {train_images_np_scaled_float32.dtype}')\n",
    "print(f'np.max(train_images_np_scaled_float32) = {np.max(train_images_np_scaled_float32)}')\n",
    "print(f'np.min(train_images_np_scaled_float32) = {np.min(train_images_np_scaled_float32)}')\n",
    "print(f'train_images_np_scaled_float32[0,:,:,0] = \\n{train_images_np_scaled_float32[0,:,:,0]}') # image 0 channel 0\n",
    "# for testing, we need both the images and the labels\n",
    "test_images_np_scaled_float32 = np.concatenate([x for x, y in test_ds_np_scaled_float32], axis=0)\n",
    "print(f'\\ntest_images_np_scaled_float32.shape = {test_images_np_scaled_float32.shape}')\n",
    "print(f'test_images_np_scaled_float32.dtype = {test_images_np_scaled_float32.dtype}')\n",
    "test_labels_np = np.concatenate([y for x, y in test_ds_np_scaled_float32], axis=0)\n",
    "print(f'test_labels_np.shape = {test_labels_np.shape}')\n",
    "print(f'test_labels_np.dtype = {test_labels_np.dtype}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=3, kernel_size=(3,3), strides=(1,1), name= 'conv2d_1',padding='same', data_format='channels_last', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), name='conv2d_2',padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), name='conv2d_3',padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), name='conv2d_4',padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "# model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), name='conv2d_5',padding='same', activation='relu'))\n",
    "# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "model.add(tf.keras.layers.Dense(units=3, activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.build(input_shape=(None,64,64,3))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "412/412 [==============================] - 56s 128ms/step - loss: 0.7864 - accuracy: 0.6310 - val_loss: 0.4281 - val_accuracy: 0.8305\n",
      "Epoch 2/5\n",
      "412/412 [==============================] - 51s 122ms/step - loss: 0.4294 - accuracy: 0.8410 - val_loss: 0.3147 - val_accuracy: 0.8817\n",
      "Epoch 3/5\n",
      "412/412 [==============================] - 51s 122ms/step - loss: 0.3358 - accuracy: 0.8796 - val_loss: 0.2665 - val_accuracy: 0.8968\n",
      "Epoch 4/5\n",
      "412/412 [==============================] - 49s 118ms/step - loss: 0.2886 - accuracy: 0.8982 - val_loss: 0.2250 - val_accuracy: 0.9159\n",
      "Epoch 5/5\n",
      "412/412 [==============================] - 49s 118ms/step - loss: 0.2496 - accuracy: 0.9136 - val_loss: 0.2025 - val_accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=5, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 - 5s - loss: 0.2379 - accuracy: 0.9167 - 5s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_ds, verbose=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# save the model as .h5\n",
    "MODEL_NAME = 'afhq_23k'\n",
    "model.save(f'trained_models_afhq/{MODEL_NAME}.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert the model to TensorFlow Lite with 8-bit full quantization\n",
    "!!! WARNING !!!: Inputs are 8bit not unsigned 8bit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/t3/v98l9wvd5v31cyynx1lwx12r0000gn/T/tmp53bzrli8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/t3/v98l9wvd5v31cyynx1lwx12r0000gn/T/tmp53bzrli8/assets\n",
      "/Users/tristantorchet/Not_iCloud/MLonMCU_Project/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-10 13:23:00.422457: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-06-10 13:23:00.423868: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-06-10 13:23:00.430100: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/t3/v98l9wvd5v31cyynx1lwx12r0000gn/T/tmp53bzrli8\n",
      "2023-06-10 13:23:00.437180: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-10 13:23:00.437208: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/t3/v98l9wvd5v31cyynx1lwx12r0000gn/T/tmp53bzrli8\n",
      "2023-06-10 13:23:00.455767: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-06-10 13:23:00.459124: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-06-10 13:23:00.590179: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/t3/v98l9wvd5v31cyynx1lwx12r0000gn/T/tmp53bzrli8\n",
      "2023-06-10 13:23:00.613630: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 184012 microseconds.\n",
      "2023-06-10 13:23:00.794920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_1_input:0\n",
      "shape: [ 1 64 64  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [1 3]\n",
      "type: <class 'numpy.int8'>\n",
      "test_images_np.shape = (1500, 64, 64, 3)\n",
      "[{'name': 'serving_default_conv2d_1_input:0', 'index': 0, 'shape': array([ 1, 64, 64,  3], dtype=int32), 'shape_signature': array([-1, 64, 64,  3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003921568859368563, -128), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "quantize = True\n",
    "if (quantize):\n",
    "    def representative_dataset():\n",
    "        for i in range(500):\n",
    "            yield([train_images_np_scaled_float32[i].reshape(1, IMG_SIZE, IMG_SIZE, 3)]) # reshape(1,156,13,1) because the model expects a batch of data (similar to torch.unsqueeze(0))\n",
    "    # Set the optimization flag.\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    # Enforce full-int8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    # Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(f'{MODEL_NAME}.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=f'{MODEL_NAME}.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "print(f'test_images_np.shape = {test_images_np_scaled_float32.shape}')\n",
    "predictions = np.zeros((test_images_np_scaled_float32.shape[0],), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "print(input_details)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_images_np_scaled_float32[0][0][0][0] = 0.47745099663734436 (float32)\n",
      "test_images_np_scaled_float32.shape = (1500, 64, 64, 3)\n",
      "test_images_np_int8[0][0][0][0] = -6 (int8)\n",
      "test_images_np_int8.shape = (1500, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'test_images_np_scaled_float32[0][0][0][0] = {test_images_np_scaled_float32[0][0][0][0]} ({test_images_np_scaled_float32.dtype})')\n",
    "print(f'test_images_np_scaled_float32.shape = {test_images_np_scaled_float32.shape}')\n",
    "test_images_np_int8 = (test_images_np_scaled_float32 / input_scale + input_zero_point).astype(input_details[0][\"dtype\"])\n",
    "print(f'test_images_np_int8[0][0][0][0] = {test_images_np_int8[0][0][0][0]} ({test_images_np_int8.dtype})')\n",
    "print(f'test_images_np_int8.shape = {test_images_np_int8.shape}')\n",
    "\n",
    "# saving the int8 images for on-mcu evaluation\n",
    "np.save('dataset_afhq/x_test_afhq.npy', test_images_np_int8)\n",
    "np.save('dataset_afhq/y_test_afhq.npy', test_labels_np)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6\n",
      "(1, 64, 64, 3)\n",
      "Sum of correct predictions is 1370\n",
      "Accuracy of quantized to int8 model is 91.33333333333333%\n",
      "Compared to float32 accuracy of 91.66666865348816%\n",
      "We have a change of -0.33333532015482614%\n"
     ]
    }
   ],
   "source": [
    "for id_img, img in enumerate(test_images_np_int8):\n",
    "    # add batch dimension and convert to int8 to match with int8 input\n",
    "    img = np.expand_dims(img, axis=0) # .astype(input_details[0][\"dtype\"])\n",
    "    if id_img== 0:\n",
    "        print(img[0][0][0][0])\n",
    "        print(img.shape)\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], img)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[id_img] = output.argmax()\n",
    "\n",
    "import time\n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels_np[i]):\n",
    "        sum = sum + 1\n",
    "        print(f'sum = {sum}', end='\\r')\n",
    "accuracy_score = sum / len(predictions)\n",
    "print(f\"Sum of correct predictions is {sum}\")\n",
    "print(f\"Accuracy of quantized to int8 model is {accuracy_score*100}%\")\n",
    "print(f\"Compared to float32 accuracy of {score[1]*100}%\")\n",
    "print(f\"We have a change of {(accuracy_score-score[1])*100}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(\n",
    "        len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data):\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str\n",
    "\n",
    "\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(f'{MODEL_NAME}.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, 'afhq'))  # 'ResNet' will be the name of the C++ object\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert the image to a txt file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0x78,\n",
      "-120\n"
     ]
    }
   ],
   "source": [
    "# We need to convert our images scaled to [0, 1] to a int8 representation\n",
    "# We use the scale and the zero point from tflite quantization details\n",
    "image = (train_images_np_scaled_float32[0] / input_scale + input_zero_point).astype(np.int8)\n",
    "cnt = 0\n",
    "first = True\n",
    "with open('dataset_afhq/afhq.txt', 'w') as f:\n",
    "    for c in range(3):\n",
    "        for i in range(IMG_SIZE):\n",
    "            for j in range(IMG_SIZE):\n",
    "                if cnt < 10:\n",
    "                    if first == True:\n",
    "                        print(f'{hex(image[i][j][c])},') # not necessary\n",
    "                        print(f'{image[i][j][c]}')\n",
    "                        first = False\n",
    "                    f.write(f'{image[i][j][c]},')\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    f.write(f'{image[i][j][c]},\\n')\n",
    "                    cnt = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
